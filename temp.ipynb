{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # data \n",
    "    # \"data_dir\":os.path.join('//mnt/d/Programs/NLP/utils/CASA-Dialogue-Act-Classifier-main', 'data'),#os.getcwd(), 'data'),\n",
    "    \"data_dir\":os.path.join(os.getcwd(),'data'),\n",
    "    \"dataset\":\"switchboard\",\n",
    "    #\"text_field\":\"clean_text\",\n",
    "    #\"label_field\":\"act_label_1\",\n",
    "    \"text_field\":\"Text\",\n",
    "    \"label_field\":\"DamslActTag\",\n",
    "\n",
    "    \"max_len\":256,\n",
    "    \"batch_size\":16, #change it to 64\n",
    "    \"num_workers\":48,\n",
    "    \n",
    "    # model\n",
    "    \"model_name\":\"roberta-base\", #roberta-base\n",
    "    \"hidden_size\":768,\n",
    "    \"num_classes\":43, # there are 43 classes in switchboard corpus\n",
    "    \n",
    "    # training\n",
    "    # \"save_dir\":os.path.join('//mnt/d/Programs/NLP/utils/CASA-Dialogue-Act-Classifier-main', 'output'),\n",
    "    \"save_dir\":os.path.join(os.getcwd(), 'output'),\n",
    "    \"project\":\"dialogue-act-classification\",\n",
    "    \"run_name\":\"context-aware-attention-dac\",\n",
    "    \"lr\":1e-5,\n",
    "    \"monitor\":\"val_accuracy\",\n",
    "    \"min_delta\":0.001,\n",
    "    \"filepath\":\"./checkpoints/{epoch}-{val_accuracy:4f}\",\n",
    "    \"precision\":32,\n",
    "    \"average\":\"micro\",\n",
    "    \"epochs\":100,\n",
    "    \"device\":torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"restart\":False,\n",
    "    \"restart_checkpoint\":\"./checkpoints/epoch=10-val_accuracy=0.720291.ckpt\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "act_label_names = {\n",
    "    'name':[\n",
    "        'Statement-non-opinion',\n",
    "        'Acknowledge (Backchannel)',\n",
    "        'Statement-opinion',\n",
    "        'Agree/Accept',\n",
    "        'Abandoned or Turn-Exit',\n",
    "        'Appreciation',\n",
    "        'Yes-No-Question',\n",
    "        'Non-verbal',\n",
    "        'Yes answers',\n",
    "        'Conventional-closing',\n",
    "        'Uninterpretable',\n",
    "        'Wh-Question',\n",
    "        'No answers',\n",
    "        'Response Acknowledgement',\n",
    "        'Hedge',\n",
    "        'Declarative Yes-No-Question',\n",
    "        'Other',\n",
    "        'Backchannel in question form',\n",
    "        'Quotation',\n",
    "        'Summarize/reformulate',\n",
    "        'Affirmative non-yes answers',\n",
    "        'Action-directive',\n",
    "        'Collaborative Completion',\n",
    "        'Repeat-phrase',\n",
    "        'Open-Question',\n",
    "        'Rhetorical-Questions',\n",
    "        'Hold before answer/agreement',\n",
    "        'Reject',\n",
    "        'Negative non-no answers',\n",
    "        'Signal-non-understanding',\n",
    "        'Other answers',\n",
    "        'Conventional-opening',\n",
    "        'Or-Clause',\n",
    "        'Dispreferred answers',\n",
    "        '3rd-party-talk',\n",
    "        'Offers, Options, Commits',\n",
    "        'Self-talk',\n",
    "        'Downplayer',\n",
    "        'Maybe/Accept-part',\n",
    "        'Tag-Question',\n",
    "        'Declarative Wh-Question',\n",
    "        'Apology',\n",
    "        'Thanking'\n",
    "    ],\n",
    "    'act_tag':[\n",
    "        'sd',\n",
    "        'b',\n",
    "        'sv',\n",
    "        'aa',\n",
    "        '%',\n",
    "        'ba',\n",
    "        'qy',\n",
    "        'x',\n",
    "        'ny',\n",
    "        'fc',\n",
    "        '%',\n",
    "        'qw',\n",
    "        'nn',\n",
    "        'bk',\n",
    "        'h',\n",
    "        'qy^d',\n",
    "        'fo_o_fw_by_bc',\n",
    "        'bh',\n",
    "        '^q',\n",
    "        'bf',\n",
    "        'na',\n",
    "        'ad',\n",
    "        '^2',\n",
    "        'b^m',\n",
    "        'qo',\n",
    "        'qh',\n",
    "        '^h',\n",
    "        'ar',\n",
    "        'ng',\n",
    "        'br',\n",
    "        'no',\n",
    "        'fp',\n",
    "        'qrr',\n",
    "        'arp_nd',\n",
    "        't3',\n",
    "        'oo_co_cc',\n",
    "        't1',\n",
    "        'bd',\n",
    "        'aap_am',\n",
    "        '^g',\n",
    "        'qw^d',\n",
    "        'fa',\n",
    "        'ft'\n",
    "    ],\n",
    "\n",
    "    'example':[\n",
    "        \"Me, I'm in the legal department.\",\n",
    "        \"Uh-huh.\",\n",
    "        \"I think it's great\",\n",
    "        \"That's exactly it.\",\n",
    "        \"So, -\",\n",
    "        \"I can imagine.\",\n",
    "        \"Do you have to have any special training?\",\n",
    "        \"[Laughter], [Throat_clearing]\",\n",
    "        \"Yes.\",\n",
    "        \"Well, it's been nice talking to you.\",\n",
    "        \"But, uh, yeah\",\n",
    "        \"Well, how old are you?\",\n",
    "        \"No.\",\n",
    "        \"Oh, okay.\",\n",
    "        \"I don't know if I'm making any sense or not.\",\n",
    "        \"So you can afford to get a house?\",\n",
    "        \"Well give me a break, you know.\",\n",
    "        \"Is that right?\",\n",
    "        \"You can't be pregnant and have cats\",\n",
    "        \"Oh, you mean you switched schools for the kids.\",\n",
    "        \"It is.\",\n",
    "        \"Why don't you go first\",\n",
    "        \"Who aren't contributing.\",\n",
    "        \"Oh, fajitas\",\n",
    "        \"How about you?\",\n",
    "        \"Who would steal a newspaper?\",\n",
    "        \"I'm drawing a blank.\",\n",
    "        \"Well, no\",\n",
    "        \"Uh, not a whole lot.\",\n",
    "        \"Excuse me?\",\n",
    "        \"I don't know\",\n",
    "        \"How are you?\",\n",
    "        \"or is it more of a company?\",\n",
    "        \"Well, not so much that.\",\n",
    "        \"My goodness, Diane, get down from there.\",\n",
    "        \"I'll have to check that out\",\n",
    "        \"What's the word I'm looking for\",\n",
    "        \"That's all right.\",\n",
    "        \"Something like that\",\n",
    "        \"Right?\",\n",
    "        \"You are what kind of buff?\",\n",
    "        \"I'm sorry.\",\n",
    "        \"Hey thanks a lot\"\n",
    "    ]\n",
    "    }\n",
    "\n",
    "class DADataset(Dataset):\n",
    "    \n",
    "    __label_dict = dict()\n",
    "    \n",
    "    def __init__(self, tokenizer, data, text_field = \"clean_text\", label_field=\"act_label_1\", max_len=512, label_dict=None, device='cpu'):\n",
    "        \n",
    "        self.text = list(data[text_field]) #data['train'][text_field]\n",
    "        self.acts = list(data[label_field]) #['train'][label_field]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "\n",
    "        if label_dict is None:\n",
    "            # build/update the label dictionary\n",
    "            classes = sorted(set(self.acts))\n",
    "        \n",
    "            for cls in classes:\n",
    "                if cls not in DADataset.__label_dict.keys():\n",
    "                    DADataset.__label_dict[cls]=len(DADataset.__label_dict.keys())\n",
    "        else:\n",
    "            DADataset.__label_dict = label_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def label_dict(self):\n",
    "        return DADataset.__label_dict\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.text[index]\n",
    "        act = self.acts[index]\n",
    "        label = DADataset.__label_dict[act]\n",
    "        \n",
    "        input_encoding = self.tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "            padding=\"max_length\",\n",
    "        ).to(self.device)\n",
    "        \n",
    "        seq_len = len(self.tokenizer.tokenize(text))\n",
    "        \n",
    "        return {\n",
    "            \"text\":text,\n",
    "            \"input_ids\":input_encoding['input_ids'].squeeze(),\n",
    "            \"attention_mask\":input_encoding['attention_mask'].squeeze(),\n",
    "            \"seq_len\":seq_len,\n",
    "            \"act\":act,\n",
    "            \"label\":torch.tensor([label], dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "# from .UtteranceRNN import UtteranceRNN\n",
    "# from .ConversationRNN import ConversationRNN\n",
    "# from .ContextAwareAttention import ContextAwareAttention\n",
    "\n",
    "\n",
    "\n",
    "class ContextAwareDAC(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name=\"roberta-base\", hidden_size=768, num_classes=18, device=torch.device(\"cpu\")):\n",
    "        \n",
    "        super(ContextAwareDAC, self).__init__()\n",
    "        \n",
    "        self.in_features = 2*hidden_size\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        # utterance encoder model\n",
    "        self.utterance_rnn = UtteranceRNN(model_name=model_name, hidden_size=hidden_size, device=device)\n",
    "        \n",
    "        # context aware self attention module\n",
    "        self.context_aware_attention = ContextAwareAttention(hidden_size=2*hidden_size, output_size=hidden_size, seq_len=128)\n",
    "        \n",
    "        # conversaton level rnn\n",
    "        self.conversation_rnn = ConversationRNN(input_size=1, hidden_size=hidden_size)\n",
    "        \n",
    "        # classifier on top of feature extractor\n",
    "        self.classifier = nn.Sequential(*[\n",
    "            nn.Linear(in_features=self.in_features, out_features=256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=128, out_features=num_classes)\n",
    "        ])\n",
    "        \n",
    "        # initial hidden_states\n",
    "        self.hx = torch.randn((2, 1, hidden_size), device=self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "            x.shape = [batch, seq_len, hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        outputs = self.utterance_rnn(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], seq_len=batch['seq_len'].tolist())\n",
    "        \n",
    "        batch = batch['input_ids'].shape[0]\n",
    "        \n",
    "        # create an empty feature vector \n",
    "        features = torch.empty((0, self.in_features), device=self.device)\n",
    "        \n",
    "        # hidden\n",
    "        hx = self.hx\n",
    "        \n",
    "    \n",
    "        for i, x in enumerate(outputs):\n",
    "            \n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "            # get sentence representation as 2d-matrix and project it linearly\n",
    "            m = self.context_aware_attention(hidden_states=x, h_forward=hx[0].detach())\n",
    "            \n",
    "            # apply rnn on linearly projected vector\n",
    "            hx = self.conversation_rnn(input_=m, hx=hx.detach())\n",
    "            \n",
    "            # concat current utterance's last hidden state to the features vector\n",
    "            features = torch.cat((features, hx.view(1, -1)), dim=0)\n",
    "            \n",
    "        \n",
    "        self.hx = hx.detach()\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConversationRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden_size=768, bidirectional=True, num_layers=1):\n",
    "        super(ConversationRNN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, input_, hx=None):\n",
    "        \n",
    "        \"\"\"\n",
    "            input_.shape = [batch, input_size] # input_size was chosen in attention module\n",
    "            hx.shape = [2, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        _, hidden = self.rnn(input=input_, hx=hx)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\n",
    "import torch\n",
    "import  torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "class UtteranceRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name=\"roberta-base\", hidden_size=768, bidirectional=True, num_layers=1, device=torch.device(\"cpu\")):\n",
    "        super(UtteranceRNN, self).__init__()\n",
    "        self.device=device\n",
    "        \n",
    "        # embedding layer is replaced by pretrained roberta's embedding\n",
    "        self.base = AutoModel.from_pretrained(pretrained_model_name_or_path=model_name, return_dict=False)\n",
    "        self.base.to(device)\n",
    "        # freeze the model parameters\n",
    "        # for param in self.base.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        #self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=hidden_size, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers, \n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, seq_len):\n",
    "        \"\"\"\n",
    "            x.shape = [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        hidden_states,_ = self.base(input_ids, attention_mask) # hidden_states.shape = [batch, max_len, hidden_size]\n",
    "        \n",
    "        # padding and packing \n",
    "        #packed_hidden_states = nn.utils.rnn.pack_padded_sequence(hidden_states, seq_len, batch_first=True, enforce_sorted=False)   \n",
    "        \n",
    "        #packed_outputs, _ = self.rnn(packed_hidden_states)\n",
    "        \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "        \n",
    "        #outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        \n",
    "        outputs,_ = self.rnn(hidden_states)\n",
    "                \n",
    "        return outputs\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ContextAwareAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size=1536, output_size=768, seq_len=128, device=torch.device(\"cpu\")):\n",
    "        super(ContextAwareAttention, self).__init__()\n",
    "        self.device=device\n",
    "        \n",
    "        # context aware self attention\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_size, out_features=output_size, bias=False)\n",
    "        self.fc_3 = nn.Linear(in_features=hidden_size//2, out_features=output_size, bias=True)\n",
    "        self.fc_2 = nn.Linear(in_features=output_size, out_features=128, bias=False)\n",
    "        \n",
    "        # linear projection\n",
    "        self.linear_projection = nn.Linear(in_features=hidden_size, out_features=1, bias=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, hidden_states, h_forward):\n",
    "        \"\"\"\n",
    "            hidden_states.shape = [batch, seq_len, hidden_size]\n",
    "            h_forward.shape = [1, hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # compute the energy\n",
    "        S = self.fc_2(torch.tanh(self.fc_1(hidden_states) + self.fc_3(h_forward.unsqueeze(1))))\n",
    "        # S.shape = [batch, seq_len, input_size] # input_size is hyperparameter\n",
    "        \n",
    "        # compute the attention\n",
    "        A = S.softmax(dim=-1)\n",
    "        \n",
    "        # Compute the sentence representation\n",
    "        M = torch.matmul(A.permute(0, 2, 1), hidden_states)\n",
    "        \n",
    "        # linear projection of the sentence\n",
    "        x = self.linear_projection(M)\n",
    "        \n",
    "        return x          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "model = ContextAwareDAC(\n",
    "    model_name=config['model_name'],\n",
    "    hidden_size=config['hidden_size'],\n",
    "    num_classes=config['num_classes'],\n",
    "    device=config['device']\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(config['data_dir'],config['dataset'], config['dataset']+\"_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-54a591ab78508fe4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\Surya\\.cache\\huggingface\\datasets\\csv\\default-54a591ab78508fe4\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 166.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\Surya\\.cache\\huggingface\\datasets\\csv\\default-54a591ab78508fe4\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 71.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_data1 = load_dataset(\"csv\", data_files=os.path.join(config['data_dir'], config['dataset'], config['dataset']+\"_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['DamslActTag', 'Text'],\n",
       "        num_rows: 193320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DADataset(\n",
    "                        tokenizer=tokenizer, \n",
    "                        data=train_data, \n",
    "                        max_len=config['max_len'], \n",
    "                        text_field=config['text_field'], \n",
    "                        label_field=config['label_field']\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "                        dataset=train_dataset, \n",
    "                        batch_size=config['batch_size'], \n",
    "                        shuffle=False, \n",
    "                        num_workers=config['num_workers']\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DamslActTag</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fo_o_fw_\"_by_bc</td>\n",
       "      <td>Okay &lt;laughter&gt;.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>&lt;Talking&gt; &lt;laughter&gt;. *slash error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qh</td>\n",
       "      <td>Where to start.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sd</td>\n",
       "      <td>I haven't had that much, of course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>%</td>\n",
       "      <td>I just heard,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193315</th>\n",
       "      <td>%</td>\n",
       "      <td>&lt;&lt;Very faint&gt;&gt; Huh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193316</th>\n",
       "      <td>sv</td>\n",
       "      <td>Uh, my job is government tooling specialist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193317</th>\n",
       "      <td>b</td>\n",
       "      <td>Oh, okay,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193318</th>\n",
       "      <td>bf</td>\n",
       "      <td>so, you know about some of these things.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193319</th>\n",
       "      <td>aa</td>\n",
       "      <td>Yeah.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193320 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DamslActTag                                          Text\n",
       "0       fo_o_fw_\"_by_bc                              Okay <laughter>.\n",
       "1                     x            <Talking> <laughter>. *slash error\n",
       "2                    qh                               Where to start.\n",
       "3                    sd            I haven't had that much, of course\n",
       "4                     %                                 I just heard,\n",
       "...                 ...                                           ...\n",
       "193315                %                           <<Very faint>> Huh.\n",
       "193316               sv  Uh, my job is government tooling specialist.\n",
       "193317                b                                     Oh, okay,\n",
       "193318               bf      so, you know about some of these things.\n",
       "193319               aa                                         Yeah.\n",
       "\n",
       "[193320 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv(os.path.join(config['data_dir'], config['dataset'], config['dataset']+\"_valid.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b0cf1dbd437a1a2e3df1a96efd3d881c8a1e8ef34b488a5521455ddfe8cc0db"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
